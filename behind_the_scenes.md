# Behind the Scenes: Dual Search Architecture with Vertex AI

## üöÄ Critical Breakthrough: Content Extraction Fix

**Problem Solved**: Vertex AI Search was finding relevant documents but returning empty content, causing poor AI responses.

**Root Cause**: Content was being extracted from the wrong location (`document.content`) instead of where Vertex AI actually stores processed content (`document.derivedStructData.extractive_segments`).

**Solution**: Implemented proper content extraction from `derivedStructData.extractive_segments`, resulting in:
- **4,899 characters** from main card documents (was 0)
- **624 characters** from welcome_benefits sections (was 0)
- **Complete document retrieval** with all policy details, rates, and conditions
- **Accurate AI responses** with detailed welcome benefits information

This fix enables Vertex AI Search to deliver the same quality as the original RAG system while maintaining enterprise-grade reliability and zero maintenance overhead.

---

## üîç What You're Seeing in the Logs

### **Phase 1A: Vertex AI Search (Primary System)**
```
INFO:src.vertex_retriever:Vertex AI Search client initialized for project: your-project
INFO:src.vertex_retriever:Vertex AI Search configured:
INFO:src.vertex_retriever:  Project: your-project
INFO:src.vertex_retriever:  Location: global
INFO:src.vertex_retriever:  Data Store: cardgpt-engine_1752662874226
INFO:src.vertex_retriever:Enhanced query with card filter: icici -> icici epm emeralde utility spends
INFO:src.vertex_retriever:Vertex AI Search completed in 3.234s, found 5 documents
```

**What's happening:**
1. **GCP Authentication**: Connect to Google Cloud Discovery Engine
2. **Data Store Access**: Connect to pre-configured Vertex AI Search data store
3. **Query Enhancement**: Add card-specific keywords for better semantic search
4. **Enterprise Search**: Google's managed search returns relevant documents
5. **Content Parsing**: Extract credit card information from derivedStructData.extractive_segments

### **Phase 1B: ChromaDB Fallback (Backup System)**
```
INFO:src.retriever:Loaded 71 documents from 3 files
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
... (repeated 71 times)
INFO:src.embedder:Generated 71/71 embeddings successfully
INFO:src.retriever:Stored 71 documents with embeddings
```

**What's happening (if Vertex AI unavailable):**
1. **JSON Parsing**: Load `axis-atlas.json`, `icici-epm.json`, and `hsbc-premier.json` files
2. **Document Creation**: Each JSON section becomes a separate "document"
3. **Individual Embedding Calls**: Each document sent separately to OpenAI embedding API
4. **Vector Storage**: Store 71 vectors in ChromaDB for future searches

### **Phase 2A: Query Processing with Vertex AI (Primary)**
```
User Query: "does icici epm give points on utility spends"

INFO:src.vertex_retriever:Enhanced query with card filter: icici -> icici epm emeralde utility spends
INFO:src.vertex_retriever:Vertex AI Search completed in 3.234s, found 5 documents
INFO:src.llm:Generated answer using gemini-1.5-flash: ~1200 input + ~150 output tokens
```

**What's happening:**
1. **Query Enhancement**: Add card-specific keywords for better semantic search
2. **Vertex AI Search**: Google's enterprise search finds relevant documents (no embeddings needed)
3. **Content Extraction**: Parse protobuf results using MessageToDict + derivedStructData.extractive_segments
4. **Smart Model Selection**: Choose optimal model based on query complexity
5. **Answer Generation**: Send enhanced question + documents to selected model (1 API call)

### **Phase 2B: Query Processing with ChromaDB (Fallback)**
```
User Query: "does icici epm give points on utility spends"

INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
INFO:src.retriever:Found 7 similar documents (threshold: 0.0)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:src.llm:Generated answer using gemini-1.5-flash: ~1200 input + ~150 output tokens
```

**What's happening (if Vertex AI unavailable):**
1. **Query Embedding**: Convert user question to vector (1 API call)
2. **Local Vector Search**: Compare query vector against 71 stored vectors using cosine similarity
3. **Context Retrieval**: Found 7 most similar documents 
4. **Smart Model Selection**: Choose optimal model based on query complexity
5. **Answer Generation**: Send enhanced question + documents to selected model (1 API call)

---

## üìã Document Creation Process

### **How JSON Files Become Documents**

**Original JSON Structure:**
```json
{
  "common_terms": {
    "interest_free_grace_period": {...},
    "finance_charges": {...},
    "rewards": {...}
  },
  "card": {
    "rewards": {...},
    "milestones": [...],
    "fees": {...}
  }
}
```

**Becomes 44 Separate Documents:**
```python
Document 1: {
  "id": "Icici_Epm_common_interest_free_grace_period",
  "cardName": "Icici Epm", 
  "section": "common_terms_interest_free_grace_period",
  "content": "Interest Free Grace Period:\n  Duration: 18 to 48 days\n  Condition For Loss: Lost if Total Amount Due is not paid..."
}

Document 2: {
  "id": "Icici_Epm_card_rewards",
  "cardName": "Icici Epm",
  "section": "rewards", 
  "content": "Rewards:\n  Rate General: 6 ICICI Bank Reward Points on every ‚Çπ200...\n  Capping Per Statement Cycle:\n    Insurance: 5,000 Reward Points..."
}
```

**Why 71 Documents from 3 Files?**
- Each major JSON section becomes a document
- ICICI EPM: ~23 sections ‚Üí 23 documents
- Axis Atlas: ~23 sections ‚Üí 23 documents  
- HSBC Premier: ~25 sections ‚Üí 25 documents
- **Total: 71 documents**

---

## üöÄ The Dual Search Pipeline Step-by-Step

### **1A. Vertex AI Search (Primary System)**

```mermaid
JSON Files ‚Üí Google Cloud Upload ‚Üí Vertex AI Indexing ‚Üí Enterprise Search
```

**Detailed Flow:**
1. **Data Upload**: Credit card JSON files uploaded to Google Cloud Discovery Engine
2. **Automatic Indexing**: Google's managed service handles document processing
3. **Enterprise Search**: Semantic search with auto-scaling infrastructure
4. **No Embeddings**: Google handles all vector operations internally
5. **Production Ready**: Auto-scaling, reliability, zero maintenance

**‚úÖ Solved: Zero Maintenance**
- No embedding generation cycles
- No prompt tuning or chunking strategies
- No result degradation over time
- Enterprise-grade reliability

### **1B. ChromaDB Fallback (Backup System)**

```mermaid
JSON Files ‚Üí Document Chunking ‚Üí Individual Embedding Calls ‚Üí ChromaDB Storage
```

**Detailed Flow:**
1. **Load JSON**: Read axis-atlas.json, icici-epm.json, hsbc-premier.json
2. **Section Extraction**: Split each card into logical sections (rewards, fees, etc.)
3. **Document Formatting**: Convert each section to readable text
4. **Embedding Generation**: **71 separate API calls** to OpenAI embeddings
5. **Vector Storage**: Store 71 vectors (1536 dimensions each) in ChromaDB

**üî¥ Inefficiency (Fallback Only): Individual Embedding Calls**
- Current: 71 separate API calls
- Could be: 1-2 batch API calls (OpenAI supports up to 2048 inputs per batch)

### **2A. Vertex AI Query Phase (Primary)**

```mermaid
User Question ‚Üí Query Enhancement ‚Üí Vertex AI Search ‚Üí Content Extraction ‚Üí LLM Answer
```

**Detailed Flow:**
1. **Query Enhancement**: "does icici epm give points on utility spends" ‚Üí "icici epm emeralde utility spends"
2. **Vertex AI Search**: Google's enterprise search finds relevant documents (no embeddings)
3. **Content Parsing**: Extract data from protobuf using MessageToDict + derivedStructData.extractive_segments
4. **Context Building**: Combine complete search results into context prompt
5. **Answer Generation**: Send context + question to selected model (1 API call)

### **2B. ChromaDB Query Phase (Fallback)**

```mermaid
User Question ‚Üí Query Embedding ‚Üí Vector Search ‚Üí Context Selection ‚Üí LLM Answer
```

**Detailed Flow:**
1. **Query Embedding**: "does icici epm give points on utility spends" ‚Üí vector (1 API call)
2. **Similarity Search**: Compare query vector vs 71 stored vectors (local, fast)
3. **Ranking**: Calculate cosine similarity scores for all documents
4. **Top-K Selection**: Pick 7 most similar documents (threshold: 0.0)
5. **Context Building**: Combine 7 documents into context prompt
6. **Answer Generation**: Send context + question to selected model (1 API call)

---

## üí° Understanding Vector Search

### **What Happens During "Local Search"**

**The 7 Documents Found:**
```
Document 1: ICICI EPM - Rewards (similarity: 0.87)
Document 2: ICICI EPM - Reward Capping (similarity: 0.84) 
Document 3: ICICI EPM - Surcharge Fees (similarity: 0.71)
Document 4: ICICI EPM - Accrual Exclusions (similarity: 0.68)
Document 5: Axis Atlas - Rewards (similarity: 0.45)
Document 6: ICICI EPM - Finance Charges (similarity: 0.42)
Document 7: ICICI EPM - Other Fees (similarity: 0.38)
```

**How Similarity Works:**
- Query: "utility spends points" ‚Üí [0.2, 0.8, 0.1, 0.9, ...]
- Doc: "Utility: 1,000 Reward Points" ‚Üí [0.3, 0.7, 0.2, 0.8, ...]
- Cosine similarity: 0.84 (high match)

### **Context Building for LLM**

**What Gets Sent to Selected Model (e.g., Gemini Flash):**
```
System: You are an expert assistant helping users understand Indian credit card terms...

User: Based on the following information, answer: "does icici epm give points on utility spends"

Context:
Card: Icici Epm
Section: rewards
Content: Rewards:
  Rate General: 6 ICICI Bank Reward Points on every ‚Çπ200 spent
  Capping Per Statement Cycle:
    Utility: 1,000 Reward Points (MCC 4900, 4899, 4901...)

Card: Icici Epm  
Section: reward_capping
Content: Reward Capping:
  Utility: 1,000 Reward Points
...
```

**Token Count Breakdown (Optimized):**
- Input: ~1,200 tokens (concise prompt + context + question)
- Output: ~150 tokens (AI's answer)
- **Cost: ~$0.0003** for this query (with Gemini Flash)

---

## üéØ Why This Dual Architecture?

### **Vertex AI Search (Primary) - Pros:**
‚úÖ **Enterprise-Grade**: Google's managed infrastructure with auto-scaling
‚úÖ **Zero Maintenance**: No prompt tuning, chunking, or result degradation cycles
‚úÖ **Fast Performance**: 2-5 seconds vs 30-60 seconds for embeddings
‚úÖ **Semantic Search**: Google's advanced search algorithms
‚úÖ **Production Ready**: 99.9% uptime with managed reliability
‚úÖ **Cost Effective**: No embedding generation costs

### **ChromaDB Fallback - Pros:**
‚úÖ **Reliable Backup**: Ensures service availability when Vertex AI unavailable
‚úÖ **Semantic Search**: Finds relevant info even with different wording
‚úÖ **Contextual Answers**: LLM sees multiple related documents  
‚úÖ **Scalable**: Can add more cards without changing code
‚úÖ **Accurate**: Combines multiple data points for comprehensive answers

### **ChromaDB Fallback - Cons (Only When Active):**
‚ùå **Slow Startup**: 71 individual embedding calls (30-60 seconds)
‚ùå **API Costs**: Each document embedding costs ~$0.000004
‚ùå **Memory Usage**: Stores all vectors in RAM
‚ùå **Maintenance**: Requires periodic optimization

---

## üöÄ Architecture Achievements

### **1. Eliminated Maintenance Cycles**
**Before (ChromaDB only):**
```python
# Endless cycles of:
# 1. Adjust chunking strategy
# 2. Tune embedding parameters
# 3. Optimize prompts
# 4. Handle result degradation
# 5. Repeat...
```

**After (Vertex AI + ChromaDB):**
```python
# Zero maintenance required:
# 1. Vertex AI handles all optimization
# 2. Google's enterprise infrastructure scales automatically
# 3. ChromaDB provides reliable fallback
# 4. Focus on business logic, not search tuning
```

**Impact:** 100% elimination of search maintenance overhead

### **2. Enterprise Reliability**
**Current:** Dual system with automatic failover
**Benefit:** 99.9% uptime with Google's managed infrastructure

### **3. Performance Optimization**
**Vertex AI:** 2-5 seconds (enterprise-optimized)
**ChromaDB Fallback:** 30-60 seconds (only when needed)
**Smart Switching:** Automatic failover ensures best performance

---

## üî¨ Query Example Breakdown

**Query:** "does icici epm give points on utility spends"

**Step 1: Query Understanding**
- Embedding captures semantic meaning: utility payments, reward points, ICICI EPM card

**Step 2: Document Matching**
- High match: "Utility: 1,000 Reward Points (MCC 4900...)"
- Medium match: "Rate General: 6 points per ‚Çπ200"
- Low match: "Surcharge fees on utilities"

**Step 3: Answer Synthesis**
AI model combines information:
- Base rate: 6 points per ‚Çπ200
- Utility capping: 1,000 points per month
- Applicable MCCs: 4900, 4899, etc.

**Final Answer:** "Yes, ICICI EPM gives 6 points per ‚Çπ200 on utility spends, capped at 1,000 points per statement cycle..."

---

## üßÆ Cost Analysis

### **One-Time Costs (Startup):**
- 46 documents √ó ~200 tokens √ó $0.00002/1K = **$0.0002**

### **Per Query Costs (Multi-Model):**
- Query embedding: ~10 tokens √ó $0.00002/1K = **$0.0000002**

| Model | Per Query Cost | Use Case |
|-------|---------------|----------|
| **Gemini Flash** | **$0.0003** | Simple queries (default) |
| **GPT-3.5** | $0.002 | General queries |
| **Gemini Pro** | $0.005 | Complex calculations |
| **GPT-4** | $0.06 | Premium accuracy |

### **Daily Usage Example (Gemini Flash):**
- 100 queries √ó $0.0003 = **$0.03/day**
- Monthly cost: **~$1** (vs $15 with GPT-3.5)

### **Cost Comparison:**
- **Gemini Flash**: $0.0003 per query (20x cheaper than GPT-3.5!)
- **GPT-3.5**: $0.002 per query  
- **GPT-4**: $0.06 per query (200x more expensive than Gemini Flash)

---

This dual architecture prioritizes **enterprise reliability** and **zero maintenance** while maintaining high accuracy and semantic understanding!

---

## üöÄ Major Enhancements Implemented

### **0. Vertex AI Search Integration (Latest)**

**Enterprise-Grade Search:**
- **Google Cloud Discovery Engine**: Managed search infrastructure
- **Auto-scaling**: Handles any query load automatically
- **Zero Maintenance**: No prompt tuning or chunking strategy cycles
- **Production Ready**: 99.9% uptime with comprehensive monitoring
- **Cost Effective**: No embedding generation costs
- **Perfect Content Extraction**: Fixed derivedStructData parsing for complete document retrieval

**Protobuf Parsing Fix:**
```python
# Problem: MapComposite objects couldn't be parsed
# Solution: Use Google's official MessageToDict method
def _convert_search_result_to_dict(result):
    return MessageToDict(result._pb)
```

**Dual System Reliability:**
- **Primary**: Vertex AI Search (enterprise-grade)
- **Fallback**: ChromaDB (reliable backup)
- **Auto-switching**: Seamless failover when needed

### **1. Multi-Model Support**

**Added Support For:**
- **OpenAI**: GPT-4, GPT-3.5-turbo
- **Google Gemini**: 1.5 Flash (ultra-cheap), 1.5 Pro (complex calculations)

**Smart Model Selection:**
```python
if complex_calculation:
    model = "gemini-1.5-pro"  # Better accuracy
else:
    model = "gemini-1.5-flash"  # 20x cheaper
```

### **2. Query Enhancement System**

**Category Detection:**
```python
query = "What rewards for ‚Çπ50K utility spend?"
# Detected: category=utility, spend_amount=50000
# Enhanced: "IMPORTANT: Axis excludes utilities, ICICI caps at 1K points"
```

**Smart Guidance:**
- **Hotel/Flight**: Use accelerated rates (5 miles vs 2 base)
- **Utility**: Check exclusions (Axis=0, ICICI=capped)
- **Complex Calc**: Auto-route to better model

### **3. Token Optimization (60% Reduction)**

**Before Optimization:**
- System prompt: 1,400 tokens
- Context: 1,200 tokens (7 docs)
- Total: ~3,000 tokens

**After Optimization:**
- System prompt: 400 tokens (condensed)
- Context: 800 tokens (4 docs, truncated)
- Total: ~1,200 tokens

**Impact:** $0.005 ‚Üí $0.0003 per query (with Gemini Flash)

### **4. Calculation Accuracy Fixes**

**Fixed Issues:**
- ‚úÖ **Milestone Logic**: Cumulative bonuses (‚Çπ7.5L = ‚Çπ3L + ‚Çπ7.5L milestones)
- ‚úÖ **Exclusion Rules**: ICICI utilities capped (not excluded)
- ‚úÖ **Arithmetic**: (20000√∑200)√ó6 = 600 (not 60)
- ‚úÖ **Category Rates**: Hotels use 5 miles/‚Çπ100 (not 2)

**Enhanced Prompts:**
```
CRITICAL: For each ‚Çπ spent, apply ONLY ONE earning rate
EXAMPLE: ‚Çπ7.5L general ‚Üí (750000√∑100)√ó2 = 15,000 + milestones 5,000 = 20,000
```